{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Logistic Regression from Scratch\n",
    "\n",
    "Scenario: After performing EDA and clustering analysis on the Maine legislative bills dataset, you want to test whether the bill's assigned committee can be predicted based on the title and text embeddings. You will implement logistic regression from scratch to perform this classification task. To keep things simple, we have picked just one category to predict (i.e., a binary classification problem). I have provided you with the boolean labels for whether a bill was assigned to the \"Housing and Economic Development\" committee in the `y.json` file.\n",
    "\n",
    "In this notebook, you will implement logistic regression from scratch using only NumPy, train it with gradient descent, and compare its performance when using `text_embedding` vs. `title_embedding` as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T00:50:28.371474Z",
     "iopub.status.busy": "2026-02-26T00:50:28.371474Z",
     "iopub.status.idle": "2026-02-26T00:50:30.012032Z",
     "shell.execute_reply": "2026-02-26T00:50:30.011356Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data processing and machine learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Importing scikit-learn tools for model building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T00:50:30.014626Z",
     "iopub.status.busy": "2026-02-26T00:50:30.014626Z",
     "iopub.status.idle": "2026-02-26T00:50:30.203502Z",
     "shell.execute_reply": "2026-02-26T00:50:30.202364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the json files from the directory  \n",
    "X_df = pd.read_json('data/X.json')\n",
    "y_df = pd.read_json('data/y.json')\n",
    "# Extracting title embeddings into properly formatted numpy arrays\n",
    "X_title = np.array(X_df['title_embedding'].tolist())\n",
    "# Extracting the text embeddings as numpy arrays\n",
    "X_text = np.array(X_df['text_embedding'].tolist())\n",
    "# Extracting the target committee assignment boolean values \n",
    "y = y_df['committee_bool'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T00:50:30.205682Z",
     "iopub.status.busy": "2026-02-26T00:50:30.205682Z",
     "iopub.status.idle": "2026-02-26T00:50:30.249200Z",
     "shell.execute_reply": "2026-02-26T00:50:30.248694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1043 samples\n",
      "Test set: 261 samples\n",
      "Positive class in train: 81 (7.8%)\n",
      "Positive class in test: 20 (7.7%)\n"
     ]
    }
   ],
   "source": [
    "# Performing 80/20 train-test split for title features, stratifying by the target label to preserve class distribution\n",
    "X_title_train, X_title_test, y_train, y_test = train_test_split(\n",
    "    X_title, y, test_size=0.2, random_state=6140, stratify=y\n",
    ")\n",
    "# Performing identical split for text features using the same random state\n",
    "X_text_train, X_text_test, _, _ = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=6140, stratify=y\n",
    ")\n",
    "\n",
    "# Initializing StandardScaler to ensure title features have 0 mean and 1 variance\n",
    "scaler_title = StandardScaler()\n",
    "X_title_train = scaler_title.fit_transform(X_title_train)\n",
    "X_title_test = scaler_title.transform(X_title_test)\n",
    "\n",
    "# Initializing and fitting the scaler for the text embeddings as well\n",
    "scaler_text = StandardScaler()\n",
    "X_text_train = scaler_text.fit_transform(X_text_train)\n",
    "X_text_test = scaler_text.transform(X_text_test)\n",
    "\n",
    "# Print out the results of our split and the class balancing\n",
    "print(f'Training set: {X_title_train.shape[0]} samples')\n",
    "print(f'Test set: {X_title_test.shape[0]} samples')\n",
    "print(f'Positive class in train: {y_train.sum()} ({y_train.mean():.1%})')\n",
    "print(f'Positive class in test: {y_test.sum()} ({y_test.mean():.1%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T00:50:30.270429Z",
     "iopub.status.busy": "2026-02-26T00:50:30.269420Z",
     "iopub.status.idle": "2026-02-26T00:50:30.280145Z",
     "shell.execute_reply": "2026-02-26T00:50:30.280145Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.theta = None  # Parameters to be learned (includes bias as first element)\n",
    "\n",
    "    # Sigmoid function squashes values to (0, 1) probability boundary\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # Injecting bias term as a column of ones into our X matrix\n",
    "    def _add_intercept(self, X):\n",
    "        return np.column_stack([np.ones(X.shape[0]), X])\n",
    "\n",
    "    # Standard batch gradient descent approach to minimize cross-entropy loss\n",
    "    def fit(self, X, y):\n",
    "        X = self._add_intercept(X)\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        # Looping through the specified number of iterations and update theta values\n",
    "        for _ in range(self.num_iterations):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.sigmoid(z)\n",
    "            # Calculating the gradient of the loss with respect to theta\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            # Adjusting weights using the learning rate and calculated gradient\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "    # Threshold raw probabilities at 0.5 to output binary prediction\n",
    "    def predict(self, X):\n",
    "        X = self._add_intercept(X)\n",
    "        z = np.dot(X, self.theta)\n",
    "        return (self.sigmoid(z) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T00:50:30.284600Z",
     "iopub.status.busy": "2026-02-26T00:50:30.283601Z",
     "iopub.status.idle": "2026-02-26T00:50:31.311683Z",
     "shell.execute_reply": "2026-02-26T00:50:31.310422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating and training the logistic regression model exclusively on text embeddings\n",
    "model_text = LogisticRegression(learning_rate=0.1, num_iterations=2000)\n",
    "model_text.fit(X_text_train, y_train)\n",
    "\n",
    "# Creating and training an identical model exclusively on title embeddings\n",
    "model_title = LogisticRegression(learning_rate=0.1, num_iterations=2000)\n",
    "model_title.fit(X_title_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T00:50:31.315927Z",
     "iopub.status.busy": "2026-02-26T00:50:31.315927Z",
     "iopub.status.idle": "2026-02-26T00:50:31.359226Z",
     "shell.execute_reply": "2026-02-26T00:50:31.358057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Text Embedding Model ===\n",
      "Accuracy:  0.9540\n",
      "Precision: 0.6818\n",
      "Recall:    0.7500\n",
      "F1-score:  0.7143\n",
      "\n",
      "=== Title Embedding Model ===\n",
      "Accuracy:  0.9157\n",
      "Precision: 0.4500\n",
      "Recall:    0.4500\n",
      "F1-score:  0.4500\n"
     ]
    }
   ],
   "source": [
    "# Predicting the binary classes for our unseen testing data using both models\n",
    "preds_text = model_text.predict(X_text_test)\n",
    "preds_title = model_title.predict(X_title_test)\n",
    "\n",
    "# Printing out evaluation metrics to compare the text vs title models\n",
    "print('=== Text Embedding Model ===')\n",
    "print(f'Accuracy:  {accuracy_score(y_test, preds_text):.4f}')\n",
    "print(f'Precision: {precision_score(y_test, preds_text):.4f}')\n",
    "print(f'Recall:    {recall_score(y_test, preds_text):.4f}')\n",
    "print(f'F1-score:  {f1_score(y_test, preds_text):.4f}')\n",
    "\n",
    "print('\\n=== Title Embedding Model ===')\n",
    "print(f'Accuracy:  {accuracy_score(y_test, preds_title):.4f}')\n",
    "print(f'Precision: {precision_score(y_test, preds_title):.4f}')\n",
    "print(f'Recall:    {recall_score(y_test, preds_title):.4f}')\n",
    "print(f'F1-score:  {f1_score(y_test, preds_title):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "1. Which feature — `text_embedding` or `title_embedding` — produced better classification performance? Why do you think that is the case?\n",
    "2. Suppose this classifier is being used to flag potential Housing bills for a human reviewer. Which error metric (accuracy, precision, recall, or F1-score) is most important in this scenario? Which is least important? Justify your answer.\n",
    "3. How does the choice of learning rate affect the convergence of gradient descent? What strategies can be used to choose an appropriate learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "1.  The text_embedding produced significantly better classification performance. Why: The text_embedding model achieved vastly superior precision and recall, leading to a much higher overall F1-score. A bill's title is very short and often relies on generic administrative phrasing, which provides the model with very little distinctive information. The full text, however, contains a massive amount of highly specific vocabulary and detailed context. This gives the Logistic Regression model the robust clues it needs to accurately assign weights and determine if the bill belongs to the target committee.\n",
    "\n",
    "2. **Most important: Recall**.If the goal is strictly to flag potential bills for a human to review, we want to make sure the human sees every single possible housing bill. A high recall score guarantees that the model catches the vast majority of the actual housing bills in the pile, even if it accidentally throws a few non-housing bills into the reviewer's inbox \n",
    "\n",
    "   **Least important: Accuracy**. The dataset is highly likely to be imbalanced since most bills are not housing bills. High accuracy could be achieved by a trivial model that simply predicts everything as 'false', making accuracy a poor and misleading metric for this use case.\n",
    "\n",
    "3.  In this highly imbalanced dataset (where over 92% of the bills are not the target class), accuracy is a deceptive metric. A useless model that simply predicts \"Not Housing\" 100% of the time would still achieve ~92% accuracy, completely failing its actual job of flagging bills for the reviewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midterm (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
