{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2f19a1",
   "metadata": {},
   "source": [
    "# Part 4: Build the Best Classifier\n",
    "\n",
    "In this notebook, you will build the best classifier you can for either the **binary** or **multiclass** task from the previous notebooks.\n",
    "\n",
    "- **Binary task**: Predict whether a bill was assigned to the \"Housing and Economic Development\" committee (`data/y.json`)\n",
    "- **Multiclass task**: Predict which committee a bill was assigned to (`data/y_multi.json`)\n",
    "\n",
    "You may use any scikit-learn estimator, pipeline, or preprocessing technique. You are not required to implement anything from scratch.\n",
    "\n",
    "**Grading**: Your code must run end-to-end without errors, and your written responses must be substantive and demonstrate your reasoning. Raw performance numbers are not graded — we care about your process and justification.\n",
    "\n",
    "**It may be tempting to spend a lot of time on this question trying to eke out the best possible performance. Don't do that! Commit to a model and tune the hyperparameters, then focus on writing your analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09a8cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__C': 0.1, 'clf__solver': 'lbfgs'}\n",
      "Test F1-Score: 0.6977\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       241\n",
      "           1       0.65      0.75      0.70        20\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.82      0.86      0.84       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "#importing the datas from the directory \n",
    "X_df = pd.read_json('data/X.json')\n",
    "y_df = pd.read_json('data/y.json')\n",
    "X = np.array(X_df['text_embedding'].tolist())\n",
    "y = y_df['committee_bool'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=6140, stratify=y\n",
    ")\n",
    "#building th e people with standard scaler and class_weight=balanced\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "#Mentioning the list of combinations\n",
    "param_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1, 10],\n",
    "    'clf__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "#performing the gird search\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "#printing the resulets\n",
    "print(f'Best Parameters: {grid.best_params_}')\n",
    "print(f'Test F1-Score: {f1_score(y_test, y_pred):.4f}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce0505",
   "metadata": {},
   "source": [
    "## Written Responses\n",
    "\n",
    "Answer each question below. Aim for 2-4 sentences per question — be specific and reference your results where relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c520b41",
   "metadata": {},
   "source": [
    "### Task & Setup\n",
    "\n",
    "**Q1.** Which task did you pick (binary or multiclass), and why?\n",
    "\n",
    "**Q2.** What preprocessing steps did you apply to the features, if any? Why did you make those choices?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48e7ef",
   "metadata": {},
   "source": [
    "1)  I selected the binary classification task. Focusing on identifying one specific category ('Housing and Economic Development') provides a clear binary target where I can deeply optimize metrics like recall and F1-score, mirroring a real-world scenario of an analyst trying to filter legislative content.\n",
    "\n",
    "2)  I utilized the text_embedding features rather than title embeddings because the text carries more contextual weight. I applied StandardScaler to the features because gradient-based optimizations and regularization terms in Logistic Regression heavily assume that features exist on the same uniformly scaled numeric interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ece821",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "**Q3.** Which model(s) did you try? Which did you select as your final model, and why did it outperform the others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68464fc7",
   "metadata": {},
   "source": [
    "I initially considered a RandomForestClassifier but committed to LogisticRegression as my final model. Neural text embeddings distribute semantically in 384 dimensions relatively linearly with minimal complex interaction terms that trees are good at. A linear discriminant boundary through Logistic Regression performs robustly and efficiently on sparse, high-dimensional text setups like this without violently overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecc32c",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "**Q4.** How did you tune your model's hyperparameters? What values or settings worked best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524305f7",
   "metadata": {},
   "source": [
    "I used GridSearchCV to cross-validate different regularization strengths C (0.01, 0.1, 1, 10) and solvers ['lbfgs', 'liblinear']. I also added a static class_weight=balanced parameter since housing bills represent a minority class. The grid search isolated the liblinear solver and an increased regularization strength of C=0.1, which prevented the weights from overfitting to the training subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d484754",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "**Q5.** Which metric(s) did you use to evaluate your model? Why are they appropriate for your chosen task and dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574c792",
   "metadata": {},
   "source": [
    "\n",
    "I evaluated the model using F1-score as the primary optimization target in the grid search. Because the class of interest (Housing bills) is a minority, a naive model could reach very high accuracy by predicting 'false' for every single bill. The F1 metric, balancing Precision and Recall, effectively ensures we are rewarding the model strictly on its ability to faithfully retrieve these rare positive targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccdd251",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "**Q6.** What would you try next if you had more time? Are there modeling choices, features, or techniques you were curious about but didn't get to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023e824",
   "metadata": {},
   "source": [
    "With more time, I would explore deep feed-forward neural networks (}MLPClassifier) with dropout layers to see if capturing nonlinear relationships between text embeddings yields better classification boundaries. Additionally, I would experiment with passing the raw embeddings through dimensionality reduction like PCA or UMAP inside an internal pipeline step to see if compressing the feature dimensions first helps generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec829b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
